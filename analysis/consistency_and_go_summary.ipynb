{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b01f0cd",
   "metadata": {},
   "source": [
    "# DeepSearch run-to-run consistency and GO coverage\n",
    "\n",
    "This notebook summarizes reproducibility between paired Perplexity DeepSearch runs and how well the inferred gene programs align with GO enrichment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "ANALYSIS_DIR = Path('analysis')\n",
    "FIG_DIR = ANALYSIS_DIR / 'figures'\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "matches = pd.read_csv(DATA_DIR / 'deepsearch_program_matches.csv')\n",
    "runs = pd.read_csv(DATA_DIR / 'deepsearch_runs.csv')\n",
    "unmatched = pd.read_csv(DATA_DIR / 'deepsearch_unmatched_programs.csv')\n",
    "duplicates = pd.read_csv(DATA_DIR / 'deepsearch_duplicate_runs.csv')\n",
    "comparison_summary = pd.read_csv(DATA_DIR / 'comparison_summary.csv')\n",
    "novel_programs = pd.read_csv(DATA_DIR / 'comparison_novel_programs.csv') if (DATA_DIR / 'comparison_novel_programs.csv').exists() else pd.DataFrame()\n",
    "\n",
    "dup_annotations = set(duplicates[duplicates['duplicate']]['annotation'])\n",
    "print(f\"Duplicate annotations flagged: {sorted(dup_annotations)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a2aec",
   "metadata": {},
   "source": [
    "## Run-to-run consistency\n",
    "\n",
    "Metrics are computed per annotation after removing duplicated runs (identical outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f86afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute per-annotation stats\n",
    "records = []\n",
    "valid_matches = matches[~matches['annotation'].isin(dup_annotations)]\n",
    "valid_runs = runs[~runs['annotation'].isin(dup_annotations)]\n",
    "valid_unmatched = unmatched[~unmatched['annotation'].isin(dup_annotations)]\n",
    "for ann in sorted(valid_runs['annotation'].unique()):\n",
    "    rc = valid_runs[valid_runs.annotation == ann].groupby('run_index')['program_count'].sum()\n",
    "    grp = valid_matches[valid_matches.annotation == ann]\n",
    "    unmatched_count = (valid_unmatched['annotation'] == ann).sum()\n",
    "    records.append({\n",
    "        'annotation': ann,\n",
    "        'run1_programs': int(rc.get(1, 0)),\n",
    "        'run2_programs': int(rc.get(2, 0)),\n",
    "        'matched_pairs': len(grp),\n",
    "        'min_programs': int(rc.min()) if not rc.empty else 0,\n",
    "        'match_rate': len(grp) / rc.min() if not rc.empty else np.nan,\n",
    "        'mean_gene_jaccard': grp['gene_jaccard'].mean() if not grp.empty else np.nan,\n",
    "        'mean_name_similarity': grp['name_similarity'].mean() if not grp.empty else np.nan,\n",
    "        'mean_combined_similarity': grp['combined_similarity'].mean() if not grp.empty else np.nan,\n",
    "        'unmatched_programs': unmatched_count,\n",
    "    })\n",
    "run_stats = pd.DataFrame(records).sort_values('annotation').reset_index(drop=True)\n",
    "run_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5947ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean = valid_matches['combined_similarity'].mean()\n",
    "overall_median = valid_matches['combined_similarity'].median()\n",
    "mean_match_rate = run_stats['match_rate'].mean()\n",
    "print(f\"Overall combined similarity: mean={overall_mean:.3f}, median={overall_median:.3f}\")\n",
    "print(f\"Average match coverage (matched pairs / smaller run): {mean_match_rate:.2f}\")\n",
    "if not run_stats.empty:\n",
    "    best_row = run_stats.loc[run_stats['mean_combined_similarity'].idxmax()]\n",
    "    worst_row = run_stats.loc[run_stats['mean_combined_similarity'].idxmin()]\n",
    "    print(f\"Best combined similarity: {best_row['annotation']} ({best_row['mean_combined_similarity']:.3f})\")\n",
    "    print(f\"Lowest combined similarity: {worst_row['annotation']} ({worst_row['mean_combined_similarity']:.3f})\")\n",
    "print(f\"Total unmatched programs (excluding duplicates): {len(valid_unmatched)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac400712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot match rates and mean similarities\n",
    "plot_order = run_stats.sort_values('mean_combined_similarity', ascending=False)['annotation']\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].bar(plot_order, run_stats.set_index('annotation').loc[plot_order, 'match_rate'], color='#4C72B0')\n",
    "axes[0].set_ylabel('Match coverage (matched pairs / smaller run)')\n",
    "axes[0].set_xlabel('Annotation')\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "axes[0].tick_params(axis='x', rotation=60)\n",
    "axes[0].set_title('Run-to-run program pairing')\n",
    "\n",
    "axes[1].bar(plot_order, run_stats.set_index('annotation').loc[plot_order, 'mean_combined_similarity'], color='#55A868')\n",
    "axes[1].set_ylabel('Mean combined similarity')\n",
    "axes[1].set_xlabel('Annotation')\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "axes[1].tick_params(axis='x', rotation=60)\n",
    "axes[1].set_title('Similarity of matched programs')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'consistency_metrics.png', dpi=200, bbox_inches='tight')\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e44c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of combined similarities per annotation\n",
    "ordered_ann = plot_order.tolist()\n",
    "subset = valid_matches.copy()\n",
    "subset['annotation'] = pd.Categorical(subset['annotation'], categories=ordered_ann, ordered=True)\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "box_data = [subset[subset['annotation'] == ann]['combined_similarity'] for ann in ordered_ann]\n",
    "ax.boxplot(box_data, labels=ordered_ann, showmeans=True, meanline=True, patch_artist=True,\n",
    "           boxprops=dict(facecolor='#CFE8F3', color='#4C72B0'),\n",
    "           medianprops=dict(color='#C44E52'),\n",
    "           meanprops=dict(color='#55A868'))\n",
    "ax.set_ylabel('Combined similarity')\n",
    "ax.set_xlabel('Annotation')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.tick_params(axis='x', rotation=60)\n",
    "ax.set_title('Distribution of combined similarity per annotation')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'combined_similarity_box.png', dpi=200, bbox_inches='tight')\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac40ab",
   "metadata": {},
   "source": [
    "## GO enrichment coverage\n",
    "\n",
    "Coverage is calculated from the comparison markdown files; sets without a parsed GO table are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_summary = comparison_summary.copy()\n",
    "go_summary['go_match_pct'] = go_summary['matched_go_terms_estimated'] / go_summary['total_gsea_terms'] * 100\n",
    "missing_annotations = sorted(set(run_stats['annotation']) - set(go_summary['annotation']))\n",
    "print('Gene sets missing GO comparison tables:', missing_annotations)\n",
    "go_summary[['annotation', 'total_gsea_terms', 'matched_go_terms_estimated', 'unmatched_go_terms_reported', 'go_match_pct']].sort_values('annotation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_terms = go_summary['total_gsea_terms'].sum()\n",
    "matched_terms = go_summary['matched_go_terms_estimated'].sum()\n",
    "unmatched_terms = go_summary['unmatched_go_terms_reported'].sum()\n",
    "print(f\"Aggregate GO coverage: {matched_terms}/{total_terms} terms matched ({matched_terms/total_terms*100:.1f}%)\")\n",
    "if not go_summary.empty:\n",
    "    min_row = go_summary.loc[go_summary['go_match_pct'].idxmin()]\n",
    "    print(f\"Lowest coverage: {min_row['annotation']} ({min_row['go_match_pct']:.1f}% of {int(min_row['total_gsea_terms'])} terms)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12732eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sorted_go = go_summary.sort_values('go_match_pct')\n",
    "ax.bar(sorted_go['annotation'], sorted_go['go_match_pct'], color='#8172B2')\n",
    "ax.set_ylabel('% GO terms matched')\n",
    "ax.set_xlabel('Annotation')\n",
    "ax.set_ylim(0, 110)\n",
    "ax.set_title('GO term coverage by gene set')\n",
    "ax.tick_params(axis='x', rotation=60)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'go_coverage_bar.png', dpi=200, bbox_inches='tight')\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b01ec",
   "metadata": {},
   "source": [
    "## Novel DeepSearch programs (no GO match)\n",
    "\n",
    "Programs listed as unmatched in the comparison files are ranked by supporting gene count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9deef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if novel_programs.empty:\n",
    "    print('No unmatched programs detected in comparison files.')\n",
    "else:\n",
    "    display_cols = ['annotation', 'program_name', 'supporting_gene_count']\n",
    "    top_novel = novel_programs.sort_values(['supporting_gene_count', 'program_name'], ascending=[False, True]).reset_index(drop=True)\n",
    "    top_novel.head(15)[display_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b331787",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- Only two DeepSearch runs per gene set; variance estimates are not robust.\n",
    "- Two metamodels produced duplicate runs, which were excluded from similarity summaries.\n",
    "- GO coverage relies on the available comparison markdowns; missing or malformed tables leave gaps.\n",
    "- Similarity metric is heuristic (50% gene Jaccard + 50% name/embedding overlap); orthogonalization of embeddings is not yet applied."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
